{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.translate import IBMModel1,AlignedSent\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EXAMPLE BASED MACHINE TRANSLATION SYSTEM 'EBMT'\n",
    "Translation in three steps: matching, alignment and recombination.<br>\n",
    "Matching: finds the example or set of examples from the bitext which most closely match the source-language string to be translated.\n",
    "Alignment: extracts the source–target translation equivalents from the retrieved examples of the matching step.<br>\n",
    "Recombination: produces the final translation by combining the target translations of the relevant subsentential fragments.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words for each languages\n",
    "en_Stp_wrd= set(stopwords.words('english'))\n",
    "tr_Stp_wrd= set(stopwords.words('turkish'))\n",
    "az_Stp_wrd= set(stopwords.words('azerbaijani'))\n",
    "stpwrds= en_Stp_wrd | tr_Stp_wrd | az_Stp_wrd #union all to have a big set of all the stop wprds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for contraction removals\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(files):\n",
    "    replacer = RegexpReplacer()\n",
    "    for file in files:\n",
    "        with open(file,'r',encoding='utf-8') as f:\n",
    "            text= f.read()\n",
    "        text = re.sub(r'[()\\[\\]{}.,;!?\\\\-]', '', text) #remove any punctutaions and unwanted characters\n",
    "        text= text.replace('[','')\n",
    "        text= text.replace(']','')\n",
    "        text=text.replace('.','')\n",
    "        text=text.replace(',','')\n",
    "        text = text.strip() # spaces \n",
    "        text=text.lower() # lower case\n",
    "        text=replacer.replace(text) # apply the stemming\n",
    "        with open(file,'w',encoding='utf-8') as f1:\n",
    "            f1.write(text) # save changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove duplicates from  a list of strings\n",
    "def remove_duplicates(string_array):\n",
    "    unique_strings = []\n",
    "    duplicates = []\n",
    "    for string in string_array:\n",
    "        words = string.split()  # Split the string into words\n",
    "        unique_words = []\n",
    "        for word in words:\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "            else:\n",
    "                duplicates.append(word)\n",
    "        unique_string = ' '.join(unique_words)\n",
    "        if unique_string not in unique_strings:\n",
    "            unique_strings.append(unique_string)\n",
    "    \n",
    "    return unique_strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a parallel corpus for 2 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_par_corpus(src_lang):\n",
    "    with open (src_lang ,'r', encoding='utf-8')as f :\n",
    "        src= f.readlines()\n",
    "    with open('Turkish.txt' ,'r' ,encoding='utf-8') as f1:\n",
    "        tk=f1.readlines()\n",
    "    src = [s.strip() for s in src]\n",
    "    tk = [t.strip() for t in tk]\n",
    "    corpus = list(zip(src,tk))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(input_sentence, src_lang):\n",
    "    target_lang = 'Turkish.txt'  # fixed\n",
    "    with open(src_lang, 'r', encoding='utf-8') as f1:\n",
    "        src_sentences = f1.readlines()\n",
    "    with open(target_lang, 'r', encoding='utf-8') as f2:\n",
    "        turkish_sentences = f2.readlines()\n",
    "\n",
    "    # Use the TF-IDF vectorizer to convert sentences to vectors\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    sentence_vectors = vectorizer.fit_transform(src_sentences + [input_sentence])\n",
    "\n",
    "    # Calculate the Cosine Similarity between the input sentence and all the other sentences\n",
    "    input_vector = sentence_vectors[-1]\n",
    "    similarity_scores = np.dot(sentence_vectors[:-1], input_vector.T).toarray().flatten()\n",
    "\n",
    "    # Sort the similarity scores and get the indices of the top 4 sentences only\n",
    "    top_indices = np.argsort(similarity_scores)[-4:]\n",
    "\n",
    "    # Retrieve the top 4 similar sentences and their Turkish translations\n",
    "    similar_sentences = [src_sentences[i] for i in top_indices]\n",
    "    turkish_sentences = [turkish_sentences[i] for i in top_indices]\n",
    "    turkish_tokens = [nltk.word_tokenize(s) for s in turkish_sentences]\n",
    "    similar_tokens = [nltk.word_tokenize(s) for s in similar_sentences]\n",
    "    parallel_sentences = list(zip(similar_tokens, turkish_tokens))\n",
    "    \n",
    "    print(\"similar sentences in the corpus: \\n\",parallel_sentences)\n",
    "    return parallel_sentences  # return the top 4 similar source sentences with their translation equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM model for checking the alignment of words\n",
    "def ibm1(src):\n",
    "    with open(src, 'r',encoding='utf-8') as f1:\n",
    "        src_sentences = f1.readlines()\n",
    "    with open('Turkish.txt', 'r',encoding='utf-8') as f2:\n",
    "        turkish_sentences = f2.readlines()\n",
    "    #extra step step to remove spaces    \n",
    "    src_sentences = [s.strip() for s in src_sentences]\n",
    "    turkish_sentences = [t.strip() for t in turkish_sentences]\n",
    "    # word tokenize all sentences\n",
    "    src_tokenized=[nltk.word_tokenize(s) for s in src_sentences]\n",
    "    tgt_tokenized =[nltk.word_tokenize(s) for s in turkish_sentences]\n",
    "    # make a parallel sentences corpus but the sentences are tokenized\n",
    "    parallel_sentences = list(zip(src_tokenized,tgt_tokenized))\n",
    "    # Align all sentnences to train the IBM model\n",
    "    bitext = [(AlignedSent(s,t)) for s,t in parallel_sentences]\n",
    "    #train the model\n",
    "    ibm1= IBMModel1(bitext,15)\n",
    "    return ibm1 # return  the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extracts the source–target translation equivalents from the retrieved examples of the matching step. \n",
    "def alignment(src,simialr_sentences,input_text):\n",
    "    ibm=ibm1(src) # create the model\n",
    "    turkish_translations= [word for sents in (pair[1] for pair in simialr_sentences) for word in sents] # extract all the turkish words from the similar sentences as the input\n",
    "    # print(turkish_translations)\n",
    "    text= nltk.word_tokenize(input_text)# tokenize the input\n",
    "    equivlants=[] # it will hold pairs of the best match in turkish for each word in the input\n",
    "    for s in text:\n",
    "        all_probs= [(ibm.translation_table[s][w]) for w in turkish_translations] # find all the probabilites of each word in the input sentence with ALL tuekish words in the similar sentences\n",
    "        for t in turkish_translations:\n",
    "            # if s in stpwrds: # if a word is a stop word, ignore it\n",
    "            #     continue\n",
    "            prob=(ibm.translation_table[s][t]) #check probabilty of the word and a turkish word\n",
    "            if prob == max(all_probs): # if its the highest probabilty , put it in pair so it acts like a dictionary\n",
    "                equivlants.append((s,t))\n",
    "                # print(f't({s}|{t} = {prob}')\n",
    "                break\n",
    "    return equivlants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function should produces the final translation by combining the target translations of the relevant subsentential fragments \n",
    "def combination(equivlants, input):\n",
    "    combined_translation=''\n",
    "    text = nltk.word_tokenize(input) #tokenize the input\n",
    "    for w in text:\n",
    "        for s,t  in equivlants:# check from the equivlants pairs we got from the alignment function\n",
    "            if w == s:  # if the word in input exists in the equiv pairs, get the translated word\n",
    "                combined_translation+=\" \"\n",
    "                combined_translation += t\n",
    "                combined_translation+=\" \"\n",
    "    return combined_translation # return the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input, src_lang): # Combine all 3 steps to produce the translation \n",
    "    flag=True # to check if the input already exist in the corpus\n",
    "    corpus= make_par_corpus(src_lang) \n",
    "    for pair  in corpus:\n",
    "        if input  == pair[0]: # if input sentence already exists, print its translation directly\n",
    "            print(\"\\ninput sentence: \\n\",input)\n",
    "            print(\"\\ntranslation: \\n\",pair[1])\n",
    "            flag=False #set flag to false to disallow rest of code to run\n",
    "            break\n",
    "    if flag:\n",
    "        similar_sentences = matching(input,src_lang)\n",
    "        align=alignment(src_lang,similar_sentences,input)\n",
    "        translation = combination(align,input)\n",
    "        str= remove_duplicates(nltk.word_tokenize(translation))\n",
    "        translation=\" \".join(str)\n",
    "        print(\"\\ninput sentence: \\n\",input)\n",
    "        print(\"\\ntranslation: \\n\",translation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar sentences in the corpus: \n",
      " [(['the', 'scent', 'of', 'freshly', 'brewed', 'tea', 'invigorates', 'your', 'senses', 'and', 'provides', 'a', 'moment', 'of', 'calm'], ['taze', 'demlenmiş', 'çayın', 'kokusu', 'duyularınızı', 'canlandırır', 've', 'sakin', 'bir', 'an', 'geçirmenizi', 'sağlar']), (['the', 'taste', 'of', 'freshly', 'baked', 'bread', 'is', 'comforting'], ['fırından', 'yeni', 'çıkmış', 'ekmeğin', 'tadı', 'insanın', 'içini', 'ısıtıyor']), (['the', 'aroma', 'of', 'freshly', 'baked', 'bread', 'brings', 'a', 'sense', 'of', 'home', 'and', 'warmth'], ['taze', 'pişmiş', 'ekmeğin', 'aroması', 'ev', 've', 'sıcaklık', 'hissi', 'verir']), (['the', 'aroma', 'of', 'freshly', 'brewed', 'tea', 'brings', 'a', 'sense', 'of', 'calm', 'and', 'relaxation'], ['taze', 'demlenmiş', 'çayın', 'aroması', 'sakinlik', 've', 'rahatlama', 'hissi', 'verir'])]\n",
      "\n",
      "input sentence: \n",
      " the smell of freshly baked bread and a brewed tea brings a sense of calm\n",
      "\n",
      "translation: \n",
      " ev kokusu taze pişmiş ekmeğin ve bir demlenmiş çayın verir sakinlik\n"
     ]
    }
   ],
   "source": [
    "files=['English.txt','Turkish.txt','Azeri.txt']\n",
    "preprocessing(files) # preprocess all the files\n",
    "translation= input('enter translation option:\\n1)English-Turkish\\n2)Azerbjiani-Turkish\\n')\n",
    "\n",
    "while(not(translation == '2' or translation == '1')):\n",
    "    translation=input(\"input 1 or 2 only\\n\\nenter translation option:\\n1)English-Turkish\\n2)Azerbjiani-Turkish\\n\")\n",
    "text = input('Enter a sentence to translate: \\n')\n",
    "while( len(text.split()) < 10): # <10\n",
    "    text=input(\"Enter a sentence of atleast 10 words:\\n\")\n",
    "\n",
    "if translation == '1':#For English-Turkish\n",
    "    translate(text, files[0])\n",
    "if translation =='2':\n",
    "    translate(text,files[2])#For Azerbjiani-Turkish  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
